{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameter(layers_n):\n",
    "    parameters = {}\n",
    "    L = len(layers_n)\n",
    "    for i in range(1,L):\n",
    "        parameters[\"W\"+str(i)] = tf.get_variable(name=\"W\"+str(i),shape=(layers_n[i],layers_n[i-1]),initializer=tf.contrib.layers.xavier_initializer())\n",
    "        parameters[\"b\"+str(i)] = tf.get_variable(name=\"b\"+str(i),shape=(layers_n[i],1),initializer=tf.zeros_initializer())\n",
    "    return parameters\n",
    "    \n",
    "#    n_hidden_1 = 256 # 1st layer number of neurons\n",
    "#    n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "#   num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "#    num_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf.reset_default_graph()\\nwith tf.Session() as sess:\\n    parameters = initialize_parameter([784,256,256,10])\\n    print(\"W1 = \" + str(parameters[\"W1\"]))\\n    print(\"b1 = \" + str(parameters[\"b1\"]))\\n    print(\"W2 = \" + str(parameters[\"W2\"]))\\n    print(\"b2 = \" + str(parameters[\"b2\"])) \\n    print(\"W3 = \" + str(parameters[\"W3\"]))\\n    print(\"b3 = \" + str(parameters[\"b3\"])) \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameter([784,256,256,10])\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"])) \n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"])) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forward(X,W,b):\n",
    "    Z = tf.add(tf.matmul(W,X),b)\n",
    "#    A = Z\n",
    "    A = tf.nn.relu(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X,parameters):\n",
    "\n",
    "    L=len(parameters)//2\n",
    "    A_prev = X\n",
    "    for i in range(1,L):\n",
    "        A = one_step_forward(A_prev , parameters[\"W\"+str(i)] , parameters[\"b\"+str(i)]) \n",
    "        A_prev = A\n",
    "    Z_last = tf.add(tf.matmul(parameters[\"W\"+str(L)],A_prev),parameters[\"b\"+str(L)])\n",
    "    return Z_last\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def com_cost(Z_last,Y):\n",
    "    logits = tf.transpose(Z_last)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(Z3,Y):\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model(layers_n,learning_rate):\n",
    "    L = len(layers_n)\n",
    "    batch_size = 128\n",
    "    num_steps = 1500\n",
    "    display_step = 100\n",
    "    X = tf.placeholder(dtype=\"float\",shape=(layers_n[0],None))\n",
    "    Y = tf.placeholder(dtype=\"float\",shape=(layers_n[L-1],None))\n",
    "    parameters = initialize_parameter(layers_n)\n",
    "    Z_last = forward(X,parameters)\n",
    "    cost = com_cost(Z_last,Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    acc = test_acc(Z_last,Y)\n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session() \n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        batch_x = batch_x.T\n",
    "        batch_y = batch_y.T\n",
    "    \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            cost_batch , acc_batch = sess.run([cost,acc], feed_dict={X: batch_x,Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                   \"{:.4f}\".format(cost_batch) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc_batch))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "        \n",
    "    X_test = mnist.test.images.T\n",
    "    Y_test = mnist.test.labels.T\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "    sess.run(acc, feed_dict={X: X_test,Y: Y_test}))\n",
    "    parameters = sess.run(parameters)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.0995, Training Accuracy= 0.336\n",
      "Step 100, Minibatch Loss= 0.2517, Training Accuracy= 0.906\n",
      "Step 200, Minibatch Loss= 0.1079, Training Accuracy= 0.969\n",
      "Step 300, Minibatch Loss= 0.1202, Training Accuracy= 0.953\n",
      "Step 400, Minibatch Loss= 0.0707, Training Accuracy= 0.992\n",
      "Step 500, Minibatch Loss= 0.1199, Training Accuracy= 0.969\n",
      "Step 600, Minibatch Loss= 0.1314, Training Accuracy= 0.945\n",
      "Step 700, Minibatch Loss= 0.1703, Training Accuracy= 0.969\n",
      "Step 800, Minibatch Loss= 0.1019, Training Accuracy= 0.969\n",
      "Step 900, Minibatch Loss= 0.0427, Training Accuracy= 0.992\n",
      "Step 1000, Minibatch Loss= 0.0602, Training Accuracy= 0.992\n",
      "Step 1100, Minibatch Loss= 0.0513, Training Accuracy= 0.992\n",
      "Step 1200, Minibatch Loss= 0.0639, Training Accuracy= 0.977\n",
      "Step 1300, Minibatch Loss= 0.0329, Training Accuracy= 1.000\n",
      "Step 1400, Minibatch Loss= 0.0213, Training Accuracy= 0.992\n",
      "Step 1500, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "layers_n = [784,256,256,10]\n",
    "learning_rate = 0.001\n",
    "parameters = full_model(layers_n,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
